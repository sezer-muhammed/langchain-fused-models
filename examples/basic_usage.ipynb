{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage of langchain-fused-model\n",
    "\n",
    "This notebook demonstrates the basic usage of the `MultiModelManager` class for managing multiple LangChain ChatModel instances.\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, make sure you have the required packages installed:\n",
    "\n",
    "```bash\n",
    "pip install langchain-fused-model langchain-openai langchain-anthropic\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the necessary modules and set up your API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_fused_model import MultiModelManager, ModelConfig, RoutingStrategy\n",
    "\n",
    "# Set your API keys (or use environment variables)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple MultiModelManager\n",
    "\n",
    "Let's create a manager with two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your models\n",
    "models = [\n",
    "    ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7),\n",
    "    ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.7),\n",
    "]\n",
    "\n",
    "# Create manager with default settings\n",
    "manager = MultiModelManager(\n",
    "    models=models,\n",
    "    strategy=RoutingStrategy.PRIORITY\n",
    ")\n",
    "\n",
    "print(\"MultiModelManager created successfully!\")\n",
    "print(f\"Managing {len(manager.models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Invocation\n",
    "\n",
    "Use the manager just like any LangChain ChatModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question\n",
    "response = manager.invoke(\"What is the capital of France?\")\n",
    "print(\"Response:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with Messages\n",
    "\n",
    "You can also use structured messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers questions concisely.\"),\n",
    "    HumanMessage(content=\"Explain quantum computing in one sentence.\")\n",
    "]\n",
    "\n",
    "response = manager.invoke(messages)\n",
    "print(\"Response:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Models\n",
    "\n",
    "Add configurations for rate limiting and priorities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configurations\n",
    "configs = [\n",
    "    ModelConfig(\n",
    "        priority=100,  # Higher priority\n",
    "        max_rpm=60,    # 60 requests per minute\n",
    "        max_rps=2,     # 2 requests per second\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        priority=50,   # Lower priority (fallback)\n",
    "        max_rpm=120,\n",
    "        max_rps=5,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create manager with configurations\n",
    "configured_manager = MultiModelManager(\n",
    "    models=models,\n",
    "    model_configs=configs,\n",
    "    strategy=RoutingStrategy.PRIORITY,\n",
    "    default_fallback=True\n",
    ")\n",
    "\n",
    "print(\"Configured manager created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Fallback Behavior\n",
    "\n",
    "Let's make multiple requests to see the manager in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"What is JavaScript?\",\n",
    "    \"What is Rust?\",\n",
    "    \"What is Go?\",\n",
    "    \"What is TypeScript?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    response = configured_manager.invoke(question)\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    print(f\"Answer: {response.content[:100]}...\")  # First 100 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Usage Statistics\n",
    "\n",
    "Check which models were used and their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get usage statistics\n",
    "stats = configured_manager._usage_tracker.get_all_stats()\n",
    "\n",
    "print(\"\\n=== Usage Statistics ===\")\n",
    "for model_idx, stat in stats.items():\n",
    "    model_type = models[model_idx]._llm_type\n",
    "    print(f\"\\nModel {model_idx} ({model_type}):\")\n",
    "    print(f\"  Total requests: {stat.total_requests}\")\n",
    "    print(f\"  Successful: {stat.successful_requests}\")\n",
    "    print(f\"  Failed: {stat.failed_requests}\")\n",
    "    \n",
    "    if stat.total_requests > 0:\n",
    "        success_rate = stat.successful_requests / stat.total_requests * 100\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"  Total tokens: {stat.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using in LangChain Chains\n",
    "\n",
    "The manager works seamlessly with LangChain chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Create a simple chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {topic}\"\n",
    ")\n",
    "\n",
    "chain = prompt | manager | StrOutputParser()\n",
    "\n",
    "# Use the chain\n",
    "result = chain.invoke({\"adjective\": \"funny\", \"topic\": \"programming\"})\n",
    "print(\"\\nJoke:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Process multiple inputs efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process multiple questions\n",
    "batch_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"What is neural network?\"\n",
    "]\n",
    "\n",
    "responses = manager.batch(batch_questions)\n",
    "\n",
    "print(\"\\n=== Batch Results ===\")\n",
    "for question, response in zip(batch_questions, responses):\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response.content[:100]}...\")  # First 100 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Creating a basic MultiModelManager\n",
    "- Configuring models with rate limits and priorities\n",
    "- Making simple and structured requests\n",
    "- Viewing usage statistics\n",
    "- Using the manager in LangChain chains\n",
    "- Batch processing\n",
    "\n",
    "For more advanced features, check out:\n",
    "- `routing_strategies.ipynb` - Different routing strategies\n",
    "- `structured_output.ipynb` - Working with Pydantic models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
