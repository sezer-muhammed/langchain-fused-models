{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing Strategies in langchain-fused-model\n",
    "\n",
    "This notebook demonstrates all available routing strategies and how to choose the right one for your use case.\n",
    "\n",
    "## Available Strategies\n",
    "\n",
    "1. **PRIORITY** - Use models in priority order\n",
    "2. **ROUND_ROBIN** - Distribute requests evenly\n",
    "3. **LEAST_USED** - Prefer models with fewest requests\n",
    "4. **COST_AWARE** - Route to lowest cost models\n",
    "5. **Custom** - Define your own strategy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_fused_model import MultiModelManager, ModelConfig, RoutingStrategy\n",
    "\n",
    "# Set your API keys\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n",
    "\n",
    "# Create three models for testing\n",
    "models = [\n",
    "    ChatOpenAI(model=\"gpt-4\", temperature=0.7),\n",
    "    ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7),\n",
    "    ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.7),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(models)} models for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Priority-Based Routing\n",
    "\n",
    "Routes to the highest priority available model. Best for:\n",
    "- Preferring premium models with fallback to cheaper alternatives\n",
    "- Quality-first approach with cost-effective fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure priorities (higher = more preferred)\n",
    "priority_configs = [\n",
    "    ModelConfig(priority=100, max_rpm=10),  # GPT-4 - highest priority, low limit\n",
    "    ModelConfig(priority=50, max_rpm=60),   # GPT-3.5 - medium priority\n",
    "    ModelConfig(priority=10, max_rpm=120),  # Claude - lowest priority\n",
    "]\n",
    "\n",
    "priority_manager = MultiModelManager(\n",
    "    models=models,\n",
    "    model_configs=priority_configs,\n",
    "    strategy=RoutingStrategy.PRIORITY,\n",
    "    default_fallback=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Priority-Based Routing ===\")\n",
    "print(\"Model priorities: GPT-4 (100) > GPT-3.5 (50) > Claude (10)\")\n",
    "\n",
    "# Make several requests\n",
    "for i in range(5):\n",
    "    response = priority_manager.invoke(f\"What is {i+1} + {i+1}?\")\n",
    "    print(f\"Request {i+1}: {response.content[:50]}\")\n",
    "\n",
    "# Check which models were used\n",
    "stats = priority_manager._usage_tracker.get_all_stats()\n",
    "print(\"\\nUsage distribution:\")\n",
    "for idx, stat in stats.items():\n",
    "    print(f\"  Model {idx}: {stat.total_requests} requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cost-Aware Routing\n",
    "\n",
    "Routes to the lowest cost model. Best for:\n",
    "- Cost optimization\n",
    "- Budget-conscious applications\n",
    "- High-volume workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure costs (per 1k tokens)\n",
    "cost_configs = [\n",
    "    ModelConfig(cost_per_1k_tokens=0.03, max_rpm=60),   # GPT-4 - expensive\n",
    "    ModelConfig(cost_per_1k_tokens=0.002, max_rpm=60),  # GPT-3.5 - cheap\n",
    "    ModelConfig(cost_per_1k_tokens=0.015, max_rpm=60),  # Claude - medium\n",
    "]\n",
    "\n",
    "cost_manager = MultiModelManager(\n",
    "    models=models,\n",
    "    model_configs=cost_configs,\n",
    "    strategy=RoutingStrategy.COST_AWARE,\n",
    "    default_fallback=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Cost-Aware Routing ===\")\n",
    "print(\"Model costs: GPT-4 ($0.03) > Claude ($0.015) > GPT-3.5 ($0.002)\")\n",
    "\n",
    "# Make several requests\n",
    "for i in range(5):\n",
    "    response = cost_manager.invoke(f\"What is the color of the sky?\")\n",
    "    print(f\"Request {i+1}: {response.content[:50]}\")\n",
    "\n",
    "# Check which models were used\n",
    "stats = cost_manager._usage_tracker.get_all_stats()\n",
    "print(\"\\nUsage distribution:\")\n",
    "for idx, stat in stats.items():\n",
    "    cost = cost_configs[idx].cost_per_1k_tokens\n",
    "    print(f\"  Model {idx} (${cost}/1k): {stat.total_requests} requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Round-Robin Routing\n",
    "\n",
    "Distributes requests evenly across models. Best for:\n",
    "- Load balancing\n",
    "- Testing multiple models\n",
    "- Avoiding rate limits on any single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple configs for round-robin\n",
    "rr_configs = [\n",
    "    ModelConfig(max_rpm=60),\n",
    "    ModelConfig(max_rpm=60),\n",
    "    ModelConfig(max_rpm=60),\n",
    "]\n",
    "\n",
    "rr_manager = MultiModelManager(\n",
    "    models=models,\n",
    "    model_configs=rr_configs,\n",
    "    strategy=RoutingStrategy.ROUND_ROBIN\n",
    ")\n",
    "\n",
    "print(\"\\n=== Round-Robin Routing ===\")\n",
    "print(\"Requests will be distributed evenly across all models\")\n",
    "\n",
    "# Make several requests\n",
    "for i in range(9):\n",
    "    response = rr_manager.invoke(f\"Count to {i+1}\")\n",
    "    print(f\"Request {i+1}: {response.content[:30]}...\")\n",
    "\n",
    "# Check distribution\n",
    "stats = rr_manager._usage_tracker.get_all_stats()\n",
    "print(\"\\nUsage distribution (should be roughly equal):\")\n",
    "for idx, stat in stats.items():\n",
    "    print(f\"  Model {idx}: {stat.total_requests} requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Least-Used Routing\n",
    "\n",
    "Routes to the model with fewest total requests. Best for:\n",
    "- Balancing usage over time\n",
    "- Avoiding overuse of any single model\n",
    "- Dynamic load distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple configs for least-used\n",
    "lu_configs = [\n",
    "    ModelConfig(max_rpm=60),\n",
    "    ModelConfig(max_rpm=60),\n",
    "    ModelConfig(max_rpm=60),\n",
    "]\n",
    "\n",
    "lu_manager = MultiModelManager(\n",
    "    models=models,\n",
    "    model_configs=lu_configs,\n",
    "    strategy=RoutingStrategy.LEAST_USED\n",
    ")\n",
    "\n",
    "print(\"\\n=== Least-Used Routing ===\")\n",
    "print(\"Each request goes to the model with fewest total requests\")\n",
    "\n",
    "# Make several requests\n",
    "for i in range(9):\n",
    "    response = lu_manager.invoke(f\"What is {i}?\")\n",
    "    print(f\"Request {i+1}: {response.content[:30]}...\")\n",
    "\n",
    "# Check distribution\n",
    "stats = lu_manager._usage_tracker.get_all_stats()\n",
    "print(\"\\nUsage distribution (should be balanced):\")\n",
    "for idx, stat in stats.items():\n",
    "    print(f\"  Model {idx}: {stat.total_requests} requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Strategy\n",
    "\n",
    "Define your own routing logic. Best for:\n",
    "- Complex business logic\n",
    "- Custom optimization criteria\n",
    "- Specialized use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_rate_strategy(models, configs, usage_stats, available_models):\n",
    "    \"\"\"\n",
    "    Custom strategy: prefer models with highest success rate.\n",
    "    Falls back to first available model if no stats yet.\n",
    "    \"\"\"\n",
    "    best_model = available_models[0]\n",
    "    best_rate = 0.0\n",
    "    \n",
    "    for idx in available_models:\n",
    "        stats = usage_stats.get(idx)\n",
    "        if stats and stats.total_requests > 0:\n",
    "            success_rate = stats.successful_requests / stats.total_requests\n",
    "            if success_rate > best_rate:\n",
    "                best_rate = success_rate\n",
    "                best_model = idx\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "custom_manager = MultiModelManager(\n",
    "    models=models,\n",
    "    model_configs=rr_configs,\n",
    "    strategy=success_rate_strategy\n",
    ")\n",
    "\n",
    "print(\"\\n=== Custom Strategy (Success Rate) ===\")\n",
    "print(\"Routes to model with highest success rate\")\n",
    "\n",
    "# Make several requests\n",
    "for i in range(6):\n",
    "    response = custom_manager.invoke(f\"Hello {i}\")\n",
    "    print(f\"Request {i+1}: {response.content[:30]}...\")\n",
    "\n",
    "# Check distribution and success rates\n",
    "stats = custom_manager._usage_tracker.get_all_stats()\n",
    "print(\"\\nUsage and success rates:\")\n",
    "for idx, stat in stats.items():\n",
    "    if stat.total_requests > 0:\n",
    "        success_rate = stat.successful_requests / stat.total_requests * 100\n",
    "        print(f\"  Model {idx}: {stat.total_requests} requests, {success_rate:.1f}% success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Comparison\n",
    "\n",
    "Let's compare all strategies side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_data = {\n",
    "    \"Strategy\": [\"PRIORITY\", \"COST_AWARE\", \"ROUND_ROBIN\", \"LEAST_USED\", \"Custom\"],\n",
    "    \"Best For\": [\n",
    "        \"Quality-first with fallback\",\n",
    "        \"Cost optimization\",\n",
    "        \"Load balancing\",\n",
    "        \"Usage balancing\",\n",
    "        \"Custom logic\"\n",
    "    ],\n",
    "    \"Distribution\": [\n",
    "        \"Uneven (by priority)\",\n",
    "        \"Uneven (by cost)\",\n",
    "        \"Even rotation\",\n",
    "        \"Balanced over time\",\n",
    "        \"Depends on logic\"\n",
    "    ],\n",
    "    \"Use Case\": [\n",
    "        \"Premium model with cheap fallback\",\n",
    "        \"High-volume, budget-conscious\",\n",
    "        \"Testing, rate limit avoidance\",\n",
    "        \"Long-running applications\",\n",
    "        \"Complex requirements\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== Strategy Comparison ===\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Strategy\n",
    "\n",
    "### Use PRIORITY when:\n",
    "- You want to prefer specific models (e.g., GPT-4) but have fallbacks\n",
    "- Quality is more important than cost\n",
    "- You have tiered model access (premium → standard → basic)\n",
    "\n",
    "### Use COST_AWARE when:\n",
    "- Cost optimization is your primary goal\n",
    "- You're processing high volumes\n",
    "- All models meet your quality requirements\n",
    "\n",
    "### Use ROUND_ROBIN when:\n",
    "- You want even distribution across models\n",
    "- Testing multiple models simultaneously\n",
    "- Avoiding rate limits on any single provider\n",
    "\n",
    "### Use LEAST_USED when:\n",
    "- You want balanced usage over time\n",
    "- Running long-term applications\n",
    "- Avoiding overuse of any single model\n",
    "\n",
    "### Use Custom when:\n",
    "- You have specific business logic\n",
    "- Need to combine multiple factors (cost + quality + availability)\n",
    "- Have unique optimization criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- All five routing strategies\n",
    "- When to use each strategy\n",
    "- How to implement custom strategies\n",
    "- Comparing strategies side by side\n",
    "\n",
    "Choose the strategy that best fits your use case, or combine multiple managers with different strategies for different parts of your application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
